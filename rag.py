import os
from dotenv import load_dotenv
from typing import List,TypedDict
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.llms import huggingface_endpoint
from langchain_community.graphs import Neo4jGraph
from langchain.chains import RetrievalQA
from langchain.chains.graph_qa.cypher import GraphCypherQAChain
from langchain.prompts import PromptTemplate
from langchain_core.documents import Document 
from langgraph.graph import StateGraph,END,START
from langgraph.checkpoint.memory import MemorySaver
load_dotenv()

EMBEDDING_MODEL="sentence-transformers/all-mpnet-base-v2"
CHROMA_PERSIST_DIRECTORY="chroma_db"
CHROMA_COLLECTION_NAME="embed_storage"

NEO4J_URI=os.getenv('NEO4J_AURA_URL')
NEO4J_USERNAME=os.getenv('NEO4J_AURA_USERNAME')
NEO4J_PASSWORD=os.getenv('NEO4J_AURA_PASSWORD')

HUGGING_FACEHUB_MODEL="meta-llama/Llama-3.1-8b-instruct"
HGFC_API_TOKEN=os.getenv("HUGGINGFACEHUB_API_TOKEN")

embedding_function=HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

vectordb=Chroma(collection_name=CHROMA_COLLECTION_NAME,embedding_function=embedding_function,persist_directory=CHROMA_PERSIST_DIRECTORY)

llm=huggingface_endpoint(repo_id=HUGGING_FACEHUB_MODEL,temperature=0.7,max_new_token=512,hugging_face_api_tokens=HGFC_API_TOKEN)

graph=Neo4jGraph(url=NEO4J_URI,username=NEO4J_USERNAME,password=NEO4J_PASSWORD)
graph.refresh_schema()

class GraphState(TypedDict):
    """
        question:The user's query.
        vector_docs:List of documents retrieved from the vector database.
        graph_result:Structured data retrieved from the graph database.
        final_answer:The final answer generated by the LLM.
        messages:A list of messages to maintain conversation history (optional).
    """

    question:str
    vector_docs:List[Document]
    graph_result:str|dict
    final_answer:str

def retrieve_from_vector_db(state:GraphState)->GraphState:
    
    question=state["question"]
    retriever=vectordb.as_retriever(search_kwargs={"k":5})
    vector_docs=retriever.invoke(question)
    print(f"Retrieved {len(vector_docs)} documents from VectorDB.")
    return {"vector_docs":vector_docs}


def query_neo4j_graph(state:GraphState)->GraphState:

    question=state["question"]
    graph_qa_chain_instance=GraphCypherQAChain.from_llm(
        llm=llm,
        graph=graph,
        verbose=True
    )

    graph_response=graph_qa_chain_instance.invoke({"query":question})
    graph_result_text=graph_response["result"]
    return {"graph_result":graph_result_text}


def synthesize_answer(state:GraphState)->GraphState:
    
    question=state["question"]
    vector_docs=state["vector_docs"]
    graph_result=state["graph_result"]

    vector_context="\n\n".join([doc.page_content for doc in vector_docs])
    full_context=f"Vector Database Context:\n{vector_context}\n\nGraph Database Context:\n{graph_result}"

    synthesis_prompt_template="""
    You are an AI assistant specializing in answering questions about academic research papers.
    You have access to two types of information:
    1.  Textual passages retrieved from documents (Vector Database Context).
    2.  Structured facts and relationships from a knowledge graph (Graph Database Context).

    Use ALL relevant information from both contexts to answer the user's question thoroughly and accurately.
    If you find conflicting information,prioritize the structured facts from the graph if they are more direct.
    If the provided contexts are insufficient to answer the question,clearly state "I cannot answer this question based on the provided information." Do not try to make up an answer.
    Cite sources by mentioning specific sections,figures,tables,or page numbers from the Vector Database Context where applicable. If the Graph Database Context provides specific names (e.g.,author names,paper titles),incorporate them directly.

    Question:{question}

    Combined Context:
    {full_context}

    Answer:\n
    """
    synthesis_prompt=PromptTemplate(
        template=synthesis_prompt_template,
        input_variables=["question","full_context"]
    )

    final_chain=synthesis_prompt | llm
    final_answer=final_chain.invoke({"question":question,"full_context":full_context}).content

    return {"final_answer":final_answer}


workflow=StateGraph(GraphState)

workflow.add_node("retrieve_vector_docs",retrieve_from_vector_db)
workflow.add_node("query_graph",query_neo4j_graph)
workflow.add_node("synthesize_answer",synthesize_answer)

workflow.add_edge(START,"retrieve_vector_docs")
workflow.add_edge("retrieve_vector_docs","query_graph")
workflow.add_edge("query_graph","synthesize_answer")
workflow.add_edge("synthesize_answer",END)
app=workflow.compile(checkpointer=MemorySaver())


if __name__=="__main__":

    while True:
        query_text=input("\nEnter your question (type 'exit' to quit):").strip()
        if query_text.lower()=='exit':
            break

        if not query_text:
            print("Please enter a question.")
            continue

        final_state=None
        for s in app.stream({"question":query_text}):
            print(f"\n--- Current State Update (Node:{list(s.keys())[0]}) ---")
            print(s)
            final_state=s

        if final_state:
            print("\n=== FINAL ANSWER ===")
            print(final_state[list(final_state.keys())[0]]["final_answer"])

            print("\n=== DEBUGGING INFORMATION ===")
            print("\nVector Source Documents:")
            retrieved_docs=final_state.get('synthesize_answer',{}).get('vector_docs',[])
            if not retrieved_docs:
                for node_output in final_state.values():
                    if isinstance(node_output,dict) and 'vector_docs' in node_output:
                        retrieved_docs=node_output['vector_docs']
                        break
            
            if retrieved_docs:
                for i,doc in enumerate(retrieved_docs):
                    print(f"  Source {i+1} (Block ID:{doc.metadata.get('block_id_original','N/A')},Page:{doc.metadata.get('page_number','N/A')},Type:{doc.metadata.get('block_type','N/A')}):")
                    print(f"    Content Snippet:{doc.page_content[:150]}...")
                    if doc.metadata.get('caption'):
                        print(f"    Caption:{doc.metadata.get('caption')[:100]}...")
                    print("-" * 20)
            else:
                print("No vector source documents were available in the final state.")
            
            print("\nGraph Raw Answer:")
            graph_answer_raw=final_state.get('synthesize_answer',{}).get('graph_result','N/A') # Might need adjustment
            if not graph_answer_raw:
                 for node_output in final_state.values():
                    if isinstance(node_output,dict) and 'graph_result' in node_output:
                        graph_answer_raw=node_output['graph_result']
                        break

            print(graph_answer_raw)